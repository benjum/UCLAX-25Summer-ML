{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f505c25",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3fb9c",
   "metadata": {},
   "source": [
    "A few common feature transformation scenarios:\n",
    "* Scaling\n",
    "* Log transform\n",
    "* Polynomial expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5bbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a526fe",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Scaling is useful for distance‑based algorithms.  Let's look at it's impact on a dataset whose features are imbalanced in scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "x, y = make_classification(n_samples=1000, n_features=2,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           flip_y=0, class_sep=1.0, random_state=0)\n",
    "\n",
    "x[:, 1] *= 1000        # increase feature‑2 scale by 3 orders of magnitude\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21904c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0],x[:,1],color=['k' if label==0 else 'r' for label in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931baf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_raw  = Pipeline([('knn', KNeighborsClassifier(n_neighbors=7))])\n",
    "\n",
    "pipe_scaled = Pipeline([('scale', StandardScaler()),\n",
    "                        ('knn',   KNeighborsClassifier(n_neighbors=7))])\n",
    "\n",
    "for name, pipe in [('No scaling', pipe_raw), ('With scaling', pipe_scaled)]:\n",
    "    pipe.fit(x_train, y_train)\n",
    "    acc = accuracy_score(y_test, pipe.predict(x_test))\n",
    "    print(f\"{name:<12s}: test accuracy = {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb481a",
   "metadata": {},
   "source": [
    "Euclidean distance is meaningless when one feature dwarfs all others; scaling fixes that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dbea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_raw  = Pipeline([('knn', LogisticRegression())])\n",
    "\n",
    "pipe_scaled = Pipeline([('scale', StandardScaler()),\n",
    "                        ('knn',   LogisticRegression())])\n",
    "\n",
    "for name, pipe in [('No scaling', pipe_raw), ('With scaling', pipe_scaled)]:\n",
    "    pipe.fit(x_train, y_train)\n",
    "    acc = accuracy_score(y_test, pipe.predict(x_test))\n",
    "    print(f\"{name:<12s}: test accuracy = {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490d268",
   "metadata": {},
   "source": [
    "## Log / power transforms\n",
    "\n",
    "These transformations can act to linearize skewed data.  Let's look at an artificial dataset that's skewed.\n",
    "\n",
    "$y = 10\\log(1+x) + noise$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e156be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x = np.random.exponential(scale=2, size=(n, 1))\n",
    "e = np.random.normal(0, 0.3, size=n)\n",
    "\n",
    "y = 10 * np.log1p(x[:, 0]) + e\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86981683",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log1p(x),y,'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d249d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log1p(x), bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6929167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "x_new = np.log1p(x_train)\n",
    "x_new_test = np.log1p(x_test)\n",
    "\n",
    "model.fit(x_new, y_train)\n",
    "\n",
    "rmse = root_mean_squared_error(y_test, model.predict(x_new_test))\n",
    "print(f\"{name:<7s}: RMSE = {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2722bd",
   "metadata": {},
   "source": [
    "LinearRegression vs. log‑transformed LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_raw = Pipeline([('lin', LinearRegression())])\n",
    "\n",
    "pipe_log = Pipeline([\n",
    "        ('log1p', FunctionTransformer(np.log1p)),\n",
    "        ('lin',   LinearRegression())\n",
    "    ])\n",
    "\n",
    "for name, pipe in [('No log', pipe_raw), ('Log1p', pipe_log)]:\n",
    "    pipe.fit(x_train, y_train)\n",
    "    rmse = root_mean_squared_error(y_test, pipe.predict(x_test))\n",
    "    print(f\"{name:<7s}: RMSE = {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231235d",
   "metadata": {},
   "source": [
    "The log transform converts a curved relationship into one the linear model can capture almost perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c426c0c2",
   "metadata": {},
   "source": [
    "# Polynomial features — adding interactions & curvature\n",
    "\n",
    "$y = (x_1 - x_2)^2 + noise$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1200\n",
    "\n",
    "x = np.random.uniform(-3, 3, size=(m, 2))\n",
    "noise = np.random.normal(0, 0.4, size=m)\n",
    "y = (x[:, 0] - x[:, 1])**2 + noise\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b398de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0],x[:,1],c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4841d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lin = Pipeline([('lin', LinearRegression())])\n",
    "\n",
    "pipe_poly = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('lin',  LinearRegression())\n",
    "    ])\n",
    "\n",
    "for name, pipe in [('Linear', pipe_lin), ('Poly d=2', pipe_poly)]:\n",
    "    pipe.fit(x_train, y_train)\n",
    "    r2 = r2_score(y_test, pipe.predict(x_test))\n",
    "    print(f\"{name:<8s}: R2 = {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f4af5f",
   "metadata": {},
   "source": [
    "PolynomialFeatures supplies squared and interaction terms that turn a previously unlearnable pattern into a linear one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365983c",
   "metadata": {},
   "source": [
    "## Transformation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca13ae",
   "metadata": {},
   "source": [
    "| Transformation          | Model type helped                   | Metric gain (above demos) |\n",
    "| ----------------------- | ----------------------------------- | ---------------------- |\n",
    "| **StandardScaler**      | Distance‑based (K‑NN, SVM, K‑Means) | Accuracy 0.48 → 0.99   |\n",
    "| **Log1p / power**       | Linear                              | RMSE 1.84 → 0.29       |\n",
    "| **Polynomial features** | Any linear model                    | R2 0.000 → 0.997       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a9b81a",
   "metadata": {},
   "source": [
    "Raw features rarely line up with an algorithm’s assumptions.\n",
    "\n",
    "Clever application of transformations helps to illuminate signals, improve convergence, and improve generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
